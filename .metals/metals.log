[0m2021.02.24 23:29:37 INFO  Started: Metals version 0.10.0 in workspace 'C:\Users\nekon\develop\courses\kafka-as-a-datahub-2021\kafka-as-a-datahub-exercices' for client vscode 1.53.2.[0m
[0m2021.02.24 23:29:40 INFO  time: initialize in 2.85s[0m
[0m2021.02.24 23:29:41 WARN  Build server is not auto-connectable.[0m
[0m2021.02.24 23:29:41 WARN  no build tool detected in workspace 'C:\Users\nekon\develop\courses\kafka-as-a-datahub-2021\kafka-as-a-datahub-exercices'. The most common cause for this problem is that the editor was opened in the wrong working directory, for example if you use sbt then the workspace directory should contain build.sbt. [0m
[0m2021.02.24 23:30:01 WARN  no build target for: C:\Users\nekon\develop\courses\kafka-as-a-datahub-2021\kafka-streams-basics\src\main\scala\org\esgi\project\Main.scala[0m
[0m2021.02.24 23:30:09 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package org.esgi.project


import java.time.Instant
import java.util.{Properties, UUID}

import akka.actor.ActorSystem
import akka.http.scaladsl.Http
import akka.http.scaladsl.model.{HttpResponse, StatusCodes}
import akka.http.scaladsl.server.Directives._
import akka.http.scaladsl.server.Route
import akka.stream.ActorMaterializer
import com.typesafe.config.{Config, ConfigFactory}
import de.heikoseeberger.akkahttpplayjson.PlayJsonSupport
import io.github.azhur.kafkaserdeplayjson.{PlayJsonSupport => KafkaStreamPlayJsonSupport}
import org.apache.kafka.streams.kstream.{KGroupedStream => _, KStream => _, KTable => _, _}
import org.apache.kafka.streams.scala.ImplicitConversions._
import org.apache.kafka.streams.scala._
import org.apache.kafka.streams.scala.kstream._
import org.apache.kafka.streams.state.{QueryableStoreTypes, ReadOnlyWindowStore, WindowStoreIterator}
import org.apache.kafka.streams.{KafkaStreams, StreamsConfig, Topology}
import org.esgi.project.models._
import org.slf4j.{Logger, LoggerFactory}

import scala.concurrent.ExecutionContextExecutor
import scala.concurrent.duration._

object Main extends PlayJsonSupport with KafkaStreamPlayJsonSupport {
  implicit val system: ActorSystem = ActorSystem.create("this-system")
  implicit val materializer: ActorMaterializer = ActorMaterializer.create(system)
  implicit val executionContext: ExecutionContextExecutor = system.dispatcher

  val logger: Logger = LoggerFactory.getLogger(this.getClass)
  val config: Config = ConfigFactory.load()

  // Configure Kafka Streams

  val props: Properties = {
    val p = new Properties()
    p.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-stream-apps-jra-teacher")
    p.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "212.47.229.218:9092")
    p
  }

  // randomize store names
  val randomUuid = UUID.randomUUID.toString
  val thirtySecondsStoreName = s"thirtySecondsVisitsStore-$randomUuid"
  val oneMinuteStoreName = s"oneMinuteVisitsStore-$randomUuid"
  val fiveMinutesStoreName = s"fiveMinuteVisitsStore-$randomUuid"

  // Run streams
  val streams: KafkaStreams = new KafkaStreams(buildProcessingGraph, props)
  streams.start()

  def buildProcessingGraph: Topology = {
    import Serdes._

    val builder: StreamsBuilder = new StreamsBuilder

    val visitsStream: KStream[String, Visit] = builder.stream[String, Visit]("visits")

    val metricsStream: KStream[String, Metric] = builder.stream[String, Metric]("metrics")

    // repartition by URL instead of random id
    val groupedByUrl: KGroupedStream[String, Visit] = visitsStream
      .map { (_, visit) =>
        (visit.url, visit)
      }
      .groupByKey

    // window per asked time frames
    val thirtySecondsWindowedVisits = groupedByUrl
      .windowedBy(TimeWindows.of(30.seconds.toMillis).advanceBy(1.second.toMillis))

    val oneMinuteWindowedVisits = groupedByUrl
      .windowedBy(TimeWindows.of(1.minute.toMillis).advanceBy(1.second.toMillis))

    val fiveMinutesWindowedVisits = groupedByUrl
      .windowedBy(TimeWindows.of(5.minute.toMillis).advanceBy(1.second.toMillis))

    case class Movie(title: String, count: Long)

    val m = Movie("Lord of the rings", 0)

    // count hits
    val thirtySecondsTable: KTable[Windowed[String], Long] = thirtySecondsWindowedVisits
      .count()(Materialized.as(thirtySecondsStoreName).withValueSerde(Serdes.Long)) // easy version

    val oneMinuteTable: KTable[Windowed[String], Long] = oneMinuteWindowedVisits
      //.aggregate(0L)((_, _, currentValue) => currentValue + 1)(Materialized.as(oneMinuteStoreName).withValueSerde(Serdes.Long)) // aggregate version
      .aggregate(Movie("", 0))((_, view, currentMovie) => currentMovie.copy(title=view.title, count=currentMovie.count + 1))

    val fiveMinuteTable: KTable[Windowed[String], Long] = fiveMinutesWindowedVisits
      .count()(Materialized.as(fiveMinutesStoreName).withValueSerde(Serdes.Long))

    // TODO: Count visits per category (second part of the URL)

    // join metrics & visits topics and create a new object which will be written to the topic augmented-metrics (use VisitWithLatency case class)
    val augmentedMetrics: KStream[String, VisitWithLatency] = visitsStream
      .join(metricsStream)(
        (visit, metric) => {
          VisitWithLatency(id = visit.id, sourceIp = visit.sourceIp, url = visit.url, timestamp = visit.timestamp, latency = metric.latency)
        },
        JoinWindows.of(30.seconds.toMillis)
      )

    // write it to Kafka
    augmentedMetrics
      .to("augmented-metrics")

    // TODO: compute mean latency per URL

    builder.build()
  }

  def routes(): Route = path("visits" / Segment) { period: String =>
    get {
      import scala.collection.JavaConverters._
      period match {
        case "30s" =>
          // load our materialized store
          val kvStore30Seconds: ReadOnlyWindowStore[String, Long] = streams.store(thirtySecondsStoreName, QueryableStoreTypes.windowStore[String, Long]())
          // fetch all available keys
          val availableKeys = kvStore30Seconds.all().asScala.map(_.key.key()).toList.distinct
          // define our time interval to fetch the last window (nearest one to now)
          val toTime = Instant.now().toEpochMilli
          val fromTime = toTime - (30 * 1000)

          complete(
            availableKeys.map { key =>
              val row: WindowStoreIterator[Long] = kvStore30Seconds.fetch(key, fromTime, toTime)
              VisitCount(url = key, count = row.asScala.toList.last.value)
            }
          )
        case "1m" =>
          // do the same for a 1 minute window
          complete(
            HttpResponse(StatusCodes.NotFound, entity = "Not found")
          )
        case "5m" =>
          // do the same for a 5 minutes window
          complete(
            HttpResponse(StatusCodes.NotFound, entity = "Not found")
          )
        case _ =>
          // unhandled period asked
          complete(
            HttpResponse(StatusCodes.NotFound, entity = "Not found")
          )
      }
    }
  }

  def main(args: Array[String]) {
    Http().bindAndHandle(routes(), "0.0.0.0", 8080)
    logger.info(s"App started on 8080")
  }
}

[0m2021.02.24 23:30:13 INFO  time: code lens generation in 11s[0m
[0m2021.02.24 23:30:13 INFO  time: code lens generation in 11s[0m
[0m2021.02.24 23:31:47 WARN  no build target for: C:\Users\nekon\develop\courses\kafka-as-a-datahub-2021\kafka-streams-basics\src\main\scala\org\esgi\project\models\Metric.scala[0m
package org.esgi.project.models

import play.api.libs.json.Json

case class Metric(
                   id: String,
                   timestamp: String,
                   latency: Int
                 )

object Metric {
  implicit val format = Json.format[Metric]
}

févr. 24, 2021 11:31:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
AVERTISSEMENT: Unmatched cancel notification for request id 21
[0m2021.02.24 23:31:58 WARN  no build target for: C:\Users\nekon\develop\courses\kafka-as-a-datahub-2021\kafka-streams-basics\src\main\scala\org\esgi\project\Main.scala[0m
[0m2021.02.24 23:38:15 WARN  no build target for: C:\Users\nekon\develop\courses\kafka-as-a-datahub-2021\kafka-streams-basics\src\main\scala\org\esgi\project\Main.scala[0m
[0m2021.02.24 23:40:57 WARN  no build target for: C:\Users\nekon\develop\courses\kafka-as-a-datahub-2021\kafka-streams-basics\src\main\scala\org\esgi\project\models\VisitCount.scala[0m
package org.esgi.project.models

import play.api.libs.json.Json

case class VisitCount(
                     url: String,
                     count: Long
                     )

object VisitCount {
  implicit val format = Json.format[VisitCount]
}

[0m2021.02.25 00:21:57 INFO  shutting down Metals[0m
2022.04.05 22:59:57 INFO  tracing is disabled for protocol LSP, to enable tracing of incoming and outgoing JSON messages create an empty file at c:\Users\nekon\develop\courses\kafka-as-a-datahub\kafka-as-a-datahub-exercices\.metals\lsp.trace.json or c:\Users\nekon\develop\courses\kafka-as-a-datahub\kafka-as-a-datahub-exercices\null\scalameta\metals\cache\lsp.trace.json
2022.04.05 22:59:58 INFO  logging to file C:\Users\nekon\develop\courses\kafka-as-a-datahub\kafka-as-a-datahub-exercices\.metals\metals.log
